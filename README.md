# whisper-benchmark
Files used for benchmarking various implementations of Whisper, done as part of my work at University of Twente + Netherlands Institute for Sound and Vision on the OH-SMArt project.

For files used for preprocessing/postprocessing of the transcriptions, check here: https://github.com/greenw0lf/OH-SMArt

Results: https://opensource-spraakherkenning-nl.github.io/ASR_NL_results/ (under NISV's Whisper benchmark).

## Python versions

`Python 3.12.3` was used for creating the virtual environments of:
- [WhisperX](https://github.com/m-bain/whisperX)
- [Huggingface (HF) `transformers`](https://huggingface.co/docs/transformers/index)

Whereas for the other implementations, `Python 3.9.18` was used:
- [faster-whisper](https://github.com/SYSTRAN/faster-whisper/)
- [faster-whisper-batched](https://github.com/SYSTRAN/faster-whisper/pull/856) (faster-whisper with batching support added)
- [OpenAI/Whisper](https://github.com/openai/whisper)
- [Whisper JAX](https://github.com/sanchit-gandhi/whisper-jax)

## Output

The output can be found in `output.tar.gz`. This contains the transcripts generated by the different Whisper implementations, logs, and stats about how much time/memory it took to complete the individual files.

The structure is as follows:
```
├── Broadcast News
└── Conversational Telephone Speech
    ├── faster-whisper
    .
    .
    .
    └── whisperx
        ├── float16
        └── float32
            ├── large-v2
            └── large-v3
                ├── labelled
                └── unlabelled
                    ├── info.csv - contains stats per file
                    ├── log.txt - log output of the notebooks
                    .
                    .
                    .
                    └── nbest_eval_..._1.json
```
